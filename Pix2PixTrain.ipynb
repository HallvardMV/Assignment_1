{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhzFEJwEX80uulQ2Chsr7Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HallvardMV/Assignment_1/blob/main/Pix2PixTrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVQN9pDR5DmD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from dataset import DatasetFromFolder\n",
        "from model import Generator, Discriminator\n",
        "import utils\n",
        "import argparse\n",
        "import os\n",
        "from logger import Logger\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', required=False, default='facades', help='input dataset')\n",
        "parser.add_argument('--direction', required=False, default='BtoA', help='input and target image order')\n",
        "parser.add_argument('--batch_size', type=int, default=1, help='train batch size')\n",
        "parser.add_argument('--ngf', type=int, default=64)\n",
        "parser.add_argument('--ndf', type=int, default=64)\n",
        "parser.add_argument('--input_size', type=int, default=256, help='input size')\n",
        "parser.add_argument('--resize_scale', type=int, default=286, help='resize scale (0 is false)')\n",
        "parser.add_argument('--crop_size', type=int, default=256, help='crop size (0 is false)')\n",
        "parser.add_argument('--fliplr', type=bool, default=True, help='random fliplr True of False')\n",
        "parser.add_argument('--num_epochs', type=int, default=200, help='number of train epochs')\n",
        "parser.add_argument('--lrG', type=float, default=0.0002, help='learning rate for generator, default=0.0002')\n",
        "parser.add_argument('--lrD', type=float, default=0.0002, help='learning rate for discriminator, default=0.0002')\n",
        "parser.add_argument('--lamb', type=float, default=100, help='lambda for L1 loss')\n",
        "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
        "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
        "params = parser.parse_args()\n",
        "print(params)\n",
        "\n",
        "# Directories for loading data and saving results\n",
        "data_dir = '../Data/' + params.dataset + '/'\n",
        "save_dir = params.dataset + '_results/'\n",
        "model_dir = params.dataset + '_model/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "# Data pre-processing\n",
        "transform = transforms.Compose([transforms.Scale(params.input_size),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
        "\n",
        "# Train data\n",
        "train_data = DatasetFromFolder(data_dir, subfolder='train', direction=params.direction, transform=transform,\n",
        "                               resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                                batch_size=params.batch_size,\n",
        "                                                shuffle=True)\n",
        "\n",
        "# Test data\n",
        "test_data = DatasetFromFolder(data_dir, subfolder='test', direction=params.direction, transform=transform)\n",
        "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                               batch_size=params.batch_size,\n",
        "                                               shuffle=False)\n",
        "test_input, test_target = test_data_loader.__iter__().__next__()\n",
        "\n",
        "# Models\n",
        "G = Generator(3, params.ngf, 3)\n",
        "D = Discriminator(6, params.ndf, 1)\n",
        "G.cuda()\n",
        "D.cuda()\n",
        "G.normal_weight_init(mean=0.0, std=0.02)\n",
        "D.normal_weight_init(mean=0.0, std=0.02)\n",
        "\n",
        "# Set the logger\n",
        "D_log_dir = save_dir + 'D_logs'\n",
        "G_log_dir = save_dir + 'G_logs'\n",
        "if not os.path.exists(D_log_dir):\n",
        "    os.mkdir(D_log_dir)\n",
        "D_logger = Logger(D_log_dir)\n",
        "\n",
        "if not os.path.exists(G_log_dir):\n",
        "    os.mkdir(G_log_dir)\n",
        "G_logger = Logger(G_log_dir)\n",
        "\n",
        "# Loss function\n",
        "BCE_loss = torch.nn.BCELoss().cuda()\n",
        "L1_loss = torch.nn.L1Loss().cuda()\n",
        "\n",
        "# Optimizers\n",
        "G_optimizer = torch.optim.Adam(G.parameters(), lr=params.lrG, betas=(params.beta1, params.beta2))\n",
        "D_optimizer = torch.optim.Adam(D.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
        "\n",
        "# Training GAN\n",
        "D_avg_losses = []\n",
        "G_avg_losses = []\n",
        "\n",
        "step = 0\n",
        "for epoch in range(params.num_epochs):\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    # training\n",
        "    for i, (input, target) in enumerate(train_data_loader):\n",
        "\n",
        "        # input & target image data\n",
        "        x_ = Variable(input.cuda())\n",
        "        y_ = Variable(target.cuda())\n",
        "\n",
        "        # Train discriminator with real data\n",
        "        D_real_decision = D(x_, y_).squeeze()\n",
        "        real_ = Variable(torch.ones(D_real_decision.size()).cuda())\n",
        "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
        "\n",
        "        # Train discriminator with fake data\n",
        "        gen_image = G(x_)\n",
        "        D_fake_decision = D(x_, gen_image).squeeze()\n",
        "        fake_ = Variable(torch.zeros(D_fake_decision.size()).cuda())\n",
        "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
        "\n",
        "        # Back propagation\n",
        "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
        "        D.zero_grad()\n",
        "        D_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        # Train generator\n",
        "        gen_image = G(x_)\n",
        "        D_fake_decision = D(x_, gen_image).squeeze()\n",
        "        G_fake_loss = BCE_loss(D_fake_decision, real_)\n",
        "\n",
        "        # L1 loss\n",
        "        l1_loss = params.lamb * L1_loss(gen_image, y_)\n",
        "\n",
        "        # Back propagation\n",
        "        G_loss = G_fake_loss + l1_loss\n",
        "        G.zero_grad()\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        # loss values\n",
        "        D_losses.append(D_loss.data[0])\n",
        "        G_losses.append(G_loss.data[0])\n",
        "\n",
        "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
        "              % (epoch+1, params.num_epochs, i+1, len(train_data_loader), D_loss.data[0], G_loss.data[0]))\n",
        "\n",
        "        # ============ TensorBoard logging ============#\n",
        "        D_logger.scalar_summary('losses', D_loss.data[0], step + 1)\n",
        "        G_logger.scalar_summary('losses', G_loss.data[0], step + 1)\n",
        "        step += 1\n",
        "\n",
        "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
        "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
        "\n",
        "    # avg loss values for plot\n",
        "    D_avg_losses.append(D_avg_loss)\n",
        "    G_avg_losses.append(G_avg_loss)\n",
        "\n",
        "    # Show result for test image\n",
        "    gen_image = G(Variable(test_input.cuda()))\n",
        "    gen_image = gen_image.cpu().data\n",
        "    utils.plot_test_result(test_input, test_target, gen_image, epoch, save=True, save_dir=save_dir)\n",
        "\n",
        "# Plot average losses\n",
        "utils.plot_loss(D_avg_losses, G_avg_losses, params.num_epochs, save=True, save_dir=save_dir)\n",
        "\n",
        "# Make gif\n",
        "utils.make_gif(params.dataset, params.num_epochs, save_dir=save_dir)\n",
        "\n",
        "# Save trained parameters of model\n",
        "torch.save(G.state_dict(), model_dir + 'generator_param.pkl')\n",
        "torch.save(D.state_dict(), model_dir + 'discriminator_param.pkl')"
      ]
    }
  ]
}